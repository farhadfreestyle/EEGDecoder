{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sexQoL1YLNXS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
        "import torch.nn.utils.parametrizations as param\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1LIj2IcLO-u"
      },
      "outputs": [],
      "source": [
        "# Load the file\n",
        "data = np.load(\"eeg_cleaned_all.npz\", allow_pickle=True)\n",
        "\n",
        "# Access EEG and labels\n",
        "eegs = data[\"X\"]   \n",
        "labels = data[\"Y\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDtfpKcvLhhF"
      },
      "outputs": [],
      "source": [
        "X_top = eegs\n",
        "Y_top = labels\n",
        "\n",
        "\n",
        "Y_top = np.array([y for y in Y_top])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCVj5KkLkq1"
      },
      "outputs": [],
      "source": [
        "class EEGTopDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.labels = y  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        # Normalize per trial \n",
        "        x = (x - x.mean(axis=1, keepdims=True)) / (x.std(axis=1, keepdims=True) + 1e-6)\n",
        "        return torch.tensor(x, dtype=torch.float32), self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRWvnZjHLkxN"
      },
      "outputs": [],
      "source": [
        "# Dataset instance\n",
        "dataset = EEGTopDataset(X_top, Y_top)\n",
        "\n",
        "# Split\n",
        "train_size = int(0.75 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_6kJR-fLk0z"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Fixed FocalBalancedLoss\n",
        "class FocalBalancedLoss(nn.Module):\n",
        "    def __init__(self, class_counts, alpha=0.25, gamma=2):\n",
        "        super().__init__()\n",
        "        weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
        "        self.weights = (weights / weights.sum()).to(device)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        log_prob = F.log_softmax(pred, dim=-1)\n",
        "        prob = torch.exp(log_prob)\n",
        "\n",
        "        # Gather the log probabilities for the target classes\n",
        "        nll_loss = -log_prob.gather(1, target.unsqueeze(1))\n",
        "\n",
        "        # Compute focal loss\n",
        "        focal_loss = self.alpha * (1 - prob.gather(1, target.unsqueeze(1)))**self.gamma * nll_loss\n",
        "\n",
        "        # Apply class weights\n",
        "        weighted_loss = focal_loss.squeeze() * self.weights[target]\n",
        "        return weighted_loss.mean()\n",
        "\n",
        "# 2. Model Architecture (unchanged)\n",
        "class AdvancedEEGNet(nn.Module):\n",
        "    def __init__(self, num_classes=80, input_shape=(62, 501)):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.channels, self.timepoints = input_shape\n",
        "\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, (1, 64), padding=(0, 32), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 128, (self.channels, 1), groups=64, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            param.weight_norm(nn.Conv2d(128, 256, (1, 32), padding=(0, 16), bias=False)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            AttentionBlock(256),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            param.weight_norm(nn.Conv2d(256, 512, (1, 16), padding=(0, 8), bias=False)),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(512, 2048),\n",
        "        nn.GELU(),\n",
        "        nn.LayerNorm(2048),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(2048, 1024),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(1024, num_classes)\n",
        "    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# 3. Attention Block\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels//8, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(channels//8, channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.conv = param.weight_norm(self.conv)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        att = self.att(x)\n",
        "        return x * att + self.conv(x)\n",
        "\n",
        "# 4. Evaluation Function\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            inputs= inputs.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# 5. Training Function\n",
        "def train(model, loader, optimizer, criterion, scaler, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
        "        inputs= inputs.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        total += labels.size(0)\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "    return running_loss / total, correct / total, grad_norm.item()\n",
        "\n",
        "# 6. Main Training Loop\n",
        "def main(train_loader, test_loader):\n",
        "    all_labels = []\n",
        "    for _, labels in train_loader:\n",
        "        all_labels.append(labels)\n",
        "    class_counts = list(Counter(torch.cat(all_labels).cpu().numpy()).values())\n",
        "\n",
        "    # Initialize model and components\n",
        "    model = AdvancedEEGNet(num_classes=80).to(device)\n",
        "    criterion = FocalBalancedLoss(class_counts)\n",
        "    optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=2, eta_min=1e-6)\n",
        "    scaler = torch.amp.GradScaler()\n",
        "    swa_start = 80\n",
        "    swa_model = AveragedModel(model)\n",
        "    swa_scheduler = SWALR(optimizer, swa_lr=1e-4)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    patience = 15\n",
        "    no_improve = 0\n",
        "\n",
        "    print(f\"Training model with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    for epoch in range(1, 201):\n",
        "        train_loss, train_acc, grad_norm = train(model, train_loader, optimizer, criterion, scaler, scheduler)\n",
        "\n",
        "        if epoch >= swa_start:\n",
        "            swa_model.update_parameters(model)\n",
        "            swa_scheduler.step()\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch}:\")\n",
        "        print(f\"Train Acc: {train_acc:.4f} | Loss: {train_loss:.4f} | Grad Norm: {grad_norm:.2f}\")\n",
        "        print(f\"Test Acc:  {test_acc:.4f} | Loss: {test_loss:.4f}\")\n",
        "        print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_modelre.pth')\n",
        "            print(\" Saved best model!\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"  Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nUpdating batch norm stats with SWA model\")\n",
        "    swa_model.to(device)\n",
        "    update_bn(train_loader, swa_model) \n",
        "    torch.save(swa_model.state_dict(), 'best_model_swa.pth')\n",
        "\n",
        "    final_test_loss, final_test_acc = evaluate(swa_model, test_loader, criterion)\n",
        "    print(f\"\\n Final SWA Test Accuracy: {final_test_acc*100:.2f}%\")\n",
        "\n",
        "    return swa_model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWc2mK_9Lk3X"
      },
      "outputs": [],
      "source": [
        "model = main(train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXapMoNYh3jv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
